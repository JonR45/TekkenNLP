{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb587b1-f78a-43d7-8d80-755bb016cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..') # this resolves ImportError: attempted relative import with no known parent package\n",
    "\n",
    "# general ds packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cleaning and pre-processing\n",
    "from src.processing.text_cleaning import (normalize_text, process_contractions, remove_all_punctuation, remove_emojis, \n",
    "remove_html_unescape, remove_href_pattern, remove_digits, remove_extra_whitespace, remove_website_links)\n",
    "\n",
    "from src.processing.text_processing import (tokenize_comment, lemmatize_comment, remove_stop_words, remove_tiny_tokens, \n",
    "remove_tekken_character_names_from_tokens, part_of_speech, part_of_speech_tag, part_of_speech_dependency, part_of_speech_shape, \n",
    "part_of_speech_alpha, part_of_speech_is_stop, word_count)\n",
    "\n",
    "# modeling\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# visualisation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a09b9e8-9482-41ec-947c-e6fffdc7f448",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69cb38f-e778-48fd-ab07-450f6824049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from csv\n",
    "raw_data = pd.read_csv(\"data/raw/new_character_reveal_comments.csv\", )\n",
    "data = raw_data.copy()\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ffe2dd-2134-48a0-a429-53fd388d4435",
   "metadata": {},
   "source": [
    "# Clean and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782dc94c-7472-4ed7-bf08-bdb7dd9d6aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.7 s, sys: 81 ms, total: 39.8 s\n",
      "Wall time: 40.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>totalReplyCount</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textDisplayWordCount</th>\n",
       "      <th>textStopWordsRemoved</th>\n",
       "      <th>textTokenized</th>\n",
       "      <th>textLemmatized</th>\n",
       "      <th>textTekkenCharactersRemoved</th>\n",
       "      <th>textProcessedCharactersRemoved</th>\n",
       "      <th>pos</th>\n",
       "      <th>posTag</th>\n",
       "      <th>posDependency</th>\n",
       "      <th>posShape</th>\n",
       "      <th>posAlpha</th>\n",
       "      <th>posStopWord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@faizaanjaved7150</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>already seen it you are getting less views now...</td>\n",
       "      <td>10</td>\n",
       "      <td>seen getting views bamco</td>\n",
       "      <td>[seen, getting, views, bamco]</td>\n",
       "      <td>[see, get, view, bamco]</td>\n",
       "      <td>[see, get, view, bamco]</td>\n",
       "      <td>see get view bamco</td>\n",
       "      <td>[VERB, VERB, NOUN, NOUN]</td>\n",
       "      <td>[VBN, VBG, NNS, NNS]</td>\n",
       "      <td>[ROOT, xcomp, dobj, dobj]</td>\n",
       "      <td>[xxxx, xxxx, xxxx, xxxx]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@TS-rw4lk</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>wow</td>\n",
       "      <td>1</td>\n",
       "      <td>wow</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>wow</td>\n",
       "      <td>[INTJ]</td>\n",
       "      <td>[UH]</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>[xxx]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@ALONCAK</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oww yeaah</td>\n",
       "      <td>2</td>\n",
       "      <td>oww yeaah</td>\n",
       "      <td>[oww, yeaah]</td>\n",
       "      <td>[oww, yeaah]</td>\n",
       "      <td>[oww, yeaah]</td>\n",
       "      <td>oww yeaah</td>\n",
       "      <td>[PROPN, PROPN]</td>\n",
       "      <td>[NNP, NNP]</td>\n",
       "      <td>[compound, ROOT]</td>\n",
       "      <td>[xxx, xxxx]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[False, False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@Rough_Estimates</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>150</td>\n",
       "      <td>18</td>\n",
       "      <td>i hope we get an angel version of jin</td>\n",
       "      <td>9</td>\n",
       "      <td>hope angel version jin</td>\n",
       "      <td>[hope, angel, version, jin]</td>\n",
       "      <td>[hope, angel, version, jin]</td>\n",
       "      <td>[hope, version]</td>\n",
       "      <td>hope version</td>\n",
       "      <td>[PROPN, PROPN, PROPN, PROPN]</td>\n",
       "      <td>[NNP, NNP, NNP, NNP]</td>\n",
       "      <td>[compound, compound, compound, ROOT]</td>\n",
       "      <td>[xxxx, xxxx, xxxx, xxx]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@kazamataurus337</td>\n",
       "      <td>2023-11-01 16:10:08+00:00</td>\n",
       "      <td>2023-11-01 16:10:08+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>so it begins</td>\n",
       "      <td>3</td>\n",
       "      <td>begins</td>\n",
       "      <td>[begins]</td>\n",
       "      <td>[begin]</td>\n",
       "      <td>[begin]</td>\n",
       "      <td>begin</td>\n",
       "      <td>[VERB]</td>\n",
       "      <td>[VBZ]</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>[xxxx]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId  authorDisplayName                publishedAt  \\\n",
       "0  rDxrpSqYHD8  @faizaanjaved7150  2023-11-01 16:10:05+00:00   \n",
       "1  rDxrpSqYHD8          @TS-rw4lk  2023-11-01 16:10:05+00:00   \n",
       "2  rDxrpSqYHD8           @ALONCAK  2023-11-01 16:10:06+00:00   \n",
       "3  rDxrpSqYHD8   @Rough_Estimates  2023-11-01 16:10:06+00:00   \n",
       "4  rDxrpSqYHD8   @kazamataurus337  2023-11-01 16:10:08+00:00   \n",
       "\n",
       "                   updatedAt  likeCount  totalReplyCount  \\\n",
       "0  2023-11-01 16:10:05+00:00          1                1   \n",
       "1  2023-11-01 16:10:05+00:00          0                0   \n",
       "2  2023-11-01 16:10:06+00:00          0                0   \n",
       "3  2023-11-01 16:10:06+00:00        150               18   \n",
       "4  2023-11-01 16:10:08+00:00          1                0   \n",
       "\n",
       "                                         textDisplay  textDisplayWordCount  \\\n",
       "0  already seen it you are getting less views now...                    10   \n",
       "1                                                wow                     1   \n",
       "2                                          oww yeaah                     2   \n",
       "3              i hope we get an angel version of jin                     9   \n",
       "4                                       so it begins                     3   \n",
       "\n",
       "       textStopWordsRemoved                  textTokenized  \\\n",
       "0  seen getting views bamco  [seen, getting, views, bamco]   \n",
       "1                       wow                          [wow]   \n",
       "2                 oww yeaah                   [oww, yeaah]   \n",
       "3    hope angel version jin    [hope, angel, version, jin]   \n",
       "4                    begins                       [begins]   \n",
       "\n",
       "                textLemmatized textTekkenCharactersRemoved  \\\n",
       "0      [see, get, view, bamco]     [see, get, view, bamco]   \n",
       "1                        [wow]                       [wow]   \n",
       "2                 [oww, yeaah]                [oww, yeaah]   \n",
       "3  [hope, angel, version, jin]             [hope, version]   \n",
       "4                      [begin]                     [begin]   \n",
       "\n",
       "  textProcessedCharactersRemoved                           pos  \\\n",
       "0             see get view bamco      [VERB, VERB, NOUN, NOUN]   \n",
       "1                            wow                        [INTJ]   \n",
       "2                      oww yeaah                [PROPN, PROPN]   \n",
       "3                   hope version  [PROPN, PROPN, PROPN, PROPN]   \n",
       "4                          begin                        [VERB]   \n",
       "\n",
       "                 posTag                         posDependency  \\\n",
       "0  [VBN, VBG, NNS, NNS]             [ROOT, xcomp, dobj, dobj]   \n",
       "1                  [UH]                                [ROOT]   \n",
       "2            [NNP, NNP]                      [compound, ROOT]   \n",
       "3  [NNP, NNP, NNP, NNP]  [compound, compound, compound, ROOT]   \n",
       "4                 [VBZ]                                [ROOT]   \n",
       "\n",
       "                   posShape                  posAlpha  \\\n",
       "0  [xxxx, xxxx, xxxx, xxxx]  [True, True, True, True]   \n",
       "1                     [xxx]                    [True]   \n",
       "2               [xxx, xxxx]              [True, True]   \n",
       "3   [xxxx, xxxx, xxxx, xxx]  [True, True, True, True]   \n",
       "4                    [xxxx]                    [True]   \n",
       "\n",
       "                    posStopWord  \n",
       "0  [False, False, False, False]  \n",
       "1                       [False]  \n",
       "2                [False, False]  \n",
       "3  [False, False, False, False]  \n",
       "4                       [False]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# clean\n",
    "df['textDisplay'] = df['textDisplay'].apply(normalize_text)\n",
    "df['textDisplay'] = df['textDisplay'].apply(process_contractions)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_website_links)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_html_unescape)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_emojis)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_digits)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_all_punctuation)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_href_pattern)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_extra_whitespace)\n",
    "\n",
    "# process\n",
    "df[\"textDisplayWordCount\"] = df['textDisplay'].apply(word_count)\n",
    "df[\"textStopWordsRemoved\"] = df[\"textDisplay\"].apply(remove_stop_words)\n",
    "df[\"textTokenized\"] = df['textStopWordsRemoved'].apply(tokenize_comment)\n",
    "df[\"textLemmatized\"] = df[\"textStopWordsRemoved\"].apply(lemmatize_comment)\n",
    "# remove short meaningless tokens from lemmatized tokens\n",
    "df[\"textLemmatized\"] = df['textLemmatized'].apply(remove_tiny_tokens)\n",
    "df[\"textTekkenCharactersRemoved\"] = df[\"textLemmatized\"].apply(remove_tekken_character_names_from_tokens)\n",
    "df[\"textProcessedCharactersRemoved\"] = df[\"textTekkenCharactersRemoved\"].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# part of speech operations\n",
    "df[\"pos\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech)\n",
    "df[\"posTag\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech_tag)\n",
    "df[\"posDependency\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech_dependency)\n",
    "df[\"posShape\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech_shape)\n",
    "df[\"posAlpha\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech_alpha)\n",
    "df[\"posStopWord\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech_is_stop)\n",
    "\n",
    "\n",
    "# remove rows with empty strings in the 'textProcessedCharactersRemoved' column as these will have nothing to pass to the vectorizer when we come to transforming the text input\n",
    "# to numerical input\n",
    "df = df[df[\"textProcessedCharactersRemoved\"].astype(str) != '']\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cef98c-3ce1-4ede-8f22-9a36029953fb",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "- We're going to build an Latent Dirichlet Allocation (LDA) model, but first we need to prepare the data so it can be read by the algorithm. To do this we need to:\n",
    "    - Create a corpus (a list that contains all of the YouTube comments in it)\n",
    "    - Create a document term matrix (a tuple of word ID and frequency of occurrence in a given document)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a047ce-8c39-40ed-9980-6d62201b194b",
   "metadata": {},
   "source": [
    "## Create a corpus and document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8251f3e6-6c4f-4f3a-9d9e-5e356ae8c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['see', 'get', 'view', 'bamco'], ['wow'], ['oww', 'yeaah'], ['hope', 'version'], ['begin'], ['waiting', 'room', 'right'], ['let'], ['wow'], ['marvelous'], ['late', 'bandai'], ['excitement'], ['marvelous'], ['new', 'trailer', 'drop', 'excited', 'game'], ['need', 'rest', 'trailer', 'mfs'], ['reason', 'get'], ['primer', 'comentario'], ['legend', 'shiiet', 'throw'], ['character', 'bandai', 'love'], ['good', 'fight', 'game', 'franchise'], ['want', 'action', 'point']]\n"
     ]
    }
   ],
   "source": [
    "# create a corpus (list of all the comments)\n",
    "corpus = list(df['textTekkenCharactersRemoved'].values)\n",
    "print(corpus[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03faf64b-40d5-4318-93d7-826dbadb86bc",
   "metadata": {},
   "source": [
    "The output shows that our corpus is a list of all the YouTube comments; the items in each list are the words that make up the comment. We now have two next steps:\n",
    "1. Use _Gensim_ to create a dictionary that will store each word in the corpus and assign a unique ID to it.\n",
    "2. Create a bag of words document-term matrix which will return a tuple with the word's unique ID and how many times it occurs in the document i.e., (word_id, frequnecy_in_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd5606f-76fb-4986-aea3-f3e751b8456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bamco\n",
      "get\n",
      "see\n"
     ]
    }
   ],
   "source": [
    "# create dictionary that will store each word in the corpus and assign a unique ID to each word\n",
    "comments_dictionary = corpora.Dictionary(corpus)\n",
    "\n",
    "# explore dictionary\n",
    "print(comments_dictionary[0])\n",
    "print(comments_dictionary[1])\n",
    "print(comments_dictionary[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6844b-fbca-4dc1-a5c3-4a810bddfec1",
   "metadata": {},
   "source": [
    "We now have a dictionary that stores the words in our corpus. Note that the first words in the dictioanry are the words in the first comment sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f87e5edd-aa0d-45e7-8dd4-687ad09b3dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 1)], [(5, 1), (6, 1)], [(7, 1), (8, 1)], [(9, 1)], [(10, 1), (11, 1), (12, 1)], [(13, 1)], [(4, 1)], [(14, 1)], [(15, 1), (16, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# create a document term matrix\n",
    "document_term_matrix = [comments_dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "print(document_term_matrix[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47156a7-994e-4ab2-9601-7c00287362fc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The document term matrix is a list of lists. Each document (nested list) is a YouTube comment represented as a list of tuples. The tuples provide the word ID (each word has a unique ID created by _Gensim_) and the frequency with which that word occurs in the document (YouTube comment).\n",
    "\n",
    "We can also view the actual word with the frequency..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbec4b20-6977-4597-9f96-3694c9d8a9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('bamco', 1), ('get', 1), ('see', 1), ('view', 1)],\n",
       " [('wow', 1)],\n",
       " [('oww', 1), ('yeaah', 1)],\n",
       " [('hope', 1), ('version', 1)],\n",
       " [('begin', 1)],\n",
       " [('right', 1), ('room', 1), ('waiting', 1)],\n",
       " [('let', 1)],\n",
       " [('wow', 1)],\n",
       " [('marvelous', 1)],\n",
       " [('bandai', 1), ('late', 1)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# human readable format of document term matrix for first 10 documents (first 10 YouTube comments)\n",
    "[[(comments_dictionary[id], freq) for id, freq in cp] for cp in document_term_matrix[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a841e6b-caad-4aae-b367-99bf54a237ff",
   "metadata": {},
   "source": [
    "We now have the data in a state where we can build the topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc89b2-0fec-45c9-9386-df438a15bebe",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6addce32-434e-4586-bea3-be9979b3fac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.063*\"want\" + 0.042*\"capoeira\" + 0.016*\"look\" + 0.016*\"game\" + 0.015*\"like\"'),\n",
       " (1,\n",
       "  '0.044*\"character\" + 0.022*\"look\" + 0.022*\"new\" + 0.020*\"game\" + 0.019*\"like\"'),\n",
       " (2,\n",
       "  '0.026*\"hope\" + 0.020*\"trailer\" + 0.014*\"come\" + 0.012*\"get\" + 0.009*\"baki\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build an LDA model\n",
    "lda_number_of_topics = 3   # manually specify the number of topics we want\n",
    "lda_model = LdaModel(corpus=document_term_matrix,\n",
    "                     num_topics=lda_number_of_topics, \n",
    "                     id2word=comments_dictionary,\n",
    "                     passes=10,   # number of iterations over the entire corpus during model training\n",
    "                     random_state=42,)\n",
    "\n",
    "# print the topics and the words most closely alighned with this topic\n",
    "lda_model.print_topics(num_topics=lda_number_of_topics, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73345fc6-1ff4-4268-ad91-e07e81970531",
   "metadata": {},
   "source": [
    "The output shows our three topics and the 5 words most closely aligned with that topic.\n",
    "The output is a list of tuples with the first item in the tuple the topic ID, and the second item contains a string with the words that make up the topic and their weighting.\n",
    "The greater the weight the more important the word is to the given topic.\n",
    "\n",
    "### Topics\n",
    "- 0 appears to focus an interest in 'capoeira', a martial art.\n",
    "- 1 appears to relate to discussion of the new characters and the 'look' or visuals of the game.\n",
    "- 2 appears focus on excitement surrounding the trailer with 'hope' showing anticipation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0816aa-3d67-4b3a-9527-2c6de253c465",
   "metadata": {},
   "source": [
    "# Visualise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8620115-d090-4b0f-a54c-4e945db69da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "viz = pyLDAvis.gensim.prepare(lda_model, document_term_matrix, comments_dictionary)\n",
    "pyLDAvis.save_html(viz, \"models/visualisations/lda_topics_viz.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e9543-f4cf-4b4c-b428-4bc4f75699e0",
   "metadata": {},
   "source": [
    "### How to interpret pyLDAvis’s output?\n",
    "(see html output for the visual)\n",
    "\n",
    "#### Bubble plot\n",
    "- Each bubble on the plot represents a topic. The larger the bubble, the more prevalent the topic.\n",
    "- A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "- A model with too many topics will typically have many overlaps and/or small bubbles clustered in one region of the chart.\n",
    "- If you move the cursor over one of the bubbles the words and bars on the right-hand side will update. These words are the most important words that form the selected topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e7b6ec-5a8c-465d-9528-dde47b70cf4e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5409bd04-b9c3-4e57-924a-40a8f98f70e7",
   "metadata": {},
   "source": [
    "### Perplexity and coherence\n",
    "Model perplexity and topic coherence provide a convenient measure to indicate how good a given topic model is.\n",
    "\n",
    "**Perplexity**:\n",
    "- The lower the perplexity better the model.\n",
    "- It is said that around 10-20 for smaller datasets and closer to 50 for larger ones is a good aim.\n",
    "- Interpretation: Very low perplexity might suggest overfitting, while excessively high values indicate poor topic capture.\n",
    "  \n",
    "**Coherence**:\n",
    "- The higher the topic coherence, the more likely the topic is to be human interpretable.\n",
    "- 'Good scores' will vary depending on the measure and dataset.\n",
    "- Measurements: For 'u_mass', above 0.5 can be good, while for 'c_v' or 'c_uci', above 0.1 might be suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2527620-a8c5-4817-bf96-8a927a5ffe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA perplexity:-7.234274494855926\n",
      "\n",
      "LDA coherence score: 0.452657714270358\n"
     ]
    }
   ],
   "source": [
    "# get perplexity score\n",
    "lda_perplexity_score = lda_model.log_perplexity(chunk=document_term_matrix, total_docs=None)   # a measure of how good the model is. lower the better.\n",
    "print(f\"\\nLDA perplexity:{lda_perplexity_score}\")   \n",
    "\n",
    "# get coherence score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus, dictionary=comments_dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f\"\\nLDA coherence score: {coherence_lda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babe6b4-e051-4bfc-b4a2-013f2bcca11e",
   "metadata": {},
   "source": [
    "- The perplexity and coherence scores are good.\n",
    "- The _pyLDAvis_ gives insight into the top 30 words for each topic. There is some overlap between topics.\n",
    "- Next we'll see if using bigrams and trigrams will improve the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e2cee3-2572-4020-95d2-7d995e1ea2f0",
   "metadata": {},
   "source": [
    "# Create corpus of bigrams and trigrams\n",
    "- Creating bigrams and trigrams can potentially better capture semantic meaning, improve topic coherence, and handle ambguity better.\n",
    "- They achieve this by:\n",
    "    - **Capturing meaning that single words might miss.** For example, \"machine learning\" or \"United States\" convey more specific concepts than \"machine\" or \"States\" alone.\n",
    "    - **Improving topic coherence by capturing more interpretable and meaningful topics**, as sometimes pairs of words can represent specific subject areas or themes better than single words.\n",
    "    - **Handling ambiguity by capturing crucial information in the second word.** For example, \"jaguar car\" vs. \"jaguar animal\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da7aa1-8b00-40ff-8c2f-e8052d9aae5f",
   "metadata": {},
   "source": [
    "## Build the bigrams and trigrams corpus\n",
    "We want to create a corpus that includes bigrams and trigrams. To do this we need to create bigram and trigram models, and to achieve this we:\n",
    "\n",
    "- Use _gensim_'s Phrases to train a model on our corpus. This model will anlyse the input corpus using statistical methods and build a model that captures frequent word co-occurrences.\n",
    "- Pass the Phrases model to _gensim_'s Phraser - this takes a trained Phrases model as input and creates an internal representation for faster phrase detection. When presented with a document, it applies the stored rules to identify and replace frequent word sequences with their corresponding phrases.\n",
    "\n",
    "### Notes\n",
    "- Experimentation is required with the _min_count_ and _threshold_ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce4f5418-6b68-41cb-99c9-46d7aa6d06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(corpus, \n",
    "                               min_count=5,   # the minimum absolute frequency a word pair needs to have in the corpus to be considered for joining into a bigram\n",
    "                               threshold=0.1)  # higher threshold leads to fewer bigrams, the bigram will need to occur at least 10 times in the document, can also\n",
    "                                              # be set as a perentage the bigram occurs in the document\n",
    "\n",
    "# use the bigram corpus to create a tigram\n",
    "trigram = gensim.models.Phrases(bigram[corpus], threshold=10)  \n",
    "\n",
    "# Phraser objects take Phrases models as inputs and are optimized for faster phrase application\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b7de61-4101-4ac4-8bfc-90805c491eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create functions that will create bigram and trigram corpora quickly\n",
    "def make_bigrams(corpus):\n",
    "    return [bigram_mod[doc] for doc in corpus]\n",
    "\n",
    "def make_trigrams(corpus):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a2cc1-dcf9-4afe-8aa0-5e755649a3fb",
   "metadata": {},
   "source": [
    "## Create bigrams corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b3a3b40-d422-48dc-a9ca-6d235b3eaf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigrams corpus\n",
    "corpus_bigrams = make_bigrams(corpus)\n",
    "\n",
    "# create dictionary\n",
    "bigrams_dictionary = corpora.Dictionary(corpus_bigrams)\n",
    "\n",
    "# create term document frequency\n",
    "bigrams_document_term_matrix = [bigrams_dictionary.doc2bow(doc) for doc in corpus_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529eb5fe-696b-4736-988c-1efcfdb12ae4",
   "metadata": {},
   "source": [
    "# Create bigrams model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db62d6a1-63cf-4b49-adb7-88a9fec96e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"wait\" + 0.012*\"legend\" + 0.012*\"not\" + 0.009*\"character\" + 0.009*\"return\"'),\n",
       " (1,\n",
       "  '0.023*\"hope\" + 0.021*\"come\" + 0.020*\"want_want\" + 0.019*\"want\" + 0.015*\"game\"'),\n",
       " (2,\n",
       "  '0.023*\"need\" + 0.020*\"look\" + 0.020*\"character\" + 0.017*\"capoeira_capoeira\" + 0.017*\"like\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA model\n",
    "lda_number_of_topics = 3\n",
    "lda_bigrams_model = LdaModel(corpus=bigrams_document_term_matrix,\n",
    "                     num_topics=lda_number_of_topics, \n",
    "                     id2word=bigrams_dictionary,\n",
    "                     passes=10,   # number of iterations over the entire corpus during model training\n",
    "                    random_state=42,)\n",
    "\n",
    "# show results\n",
    "lda_bigrams_model.print_topics(num_topics=lda_number_of_topics, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca702f-44c5-45a7-a476-30b1120732bb",
   "metadata": {},
   "source": [
    "The output shows our three topics and the 5 words most closely aligned with that topic.\n",
    "The output is a list of tuples with the first item in the tuple the topic ID, and the second item contains a string with the words that make up the topic and their weighting.\n",
    "The greater the weight the more important the word is to the given topic.\n",
    "\n",
    "### Topics\n",
    "- 0 appears to focus on returning characters and 'legends' of the game.\n",
    "- 1 appears to show excitement for the upcoming game.\n",
    "- 2 appears focus on capoeira - a martial art - and the visual appearance of this fighting style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13788adf-3c78-4083-af8a-4a3959b9a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "viz = pyLDAvis.gensim.prepare(lda_bigrams_model, bigrams_document_term_matrix, bigrams_dictionary)\n",
    "pyLDAvis.save_html(viz, \"models/visualisations/lda_topics_bigrams_viz.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefc88b-8e25-4ba2-89cc-81831d53d2ae",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be13ef77-c11e-453a-a640-67f882f9d3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA bigrams perplexity: -7.465235348984373\n",
      "\n",
      "LDA bigrams coherence score: 0.44210836940752\n"
     ]
    }
   ],
   "source": [
    "# get perplexity score\n",
    "lda_bigrams_perplexity_score = lda_bigrams_model.log_perplexity(chunk=bigrams_document_term_matrix, total_docs=None)\n",
    "print(f\"\\nLDA bigrams perplexity: {lda_bigrams_perplexity_score}\")   # a measure of how good the model is. lower the better.\n",
    "\n",
    "# get coherence Score\n",
    "coherence_model_lda_bigrams = CoherenceModel(model=lda_bigrams_model, texts=corpus_bigrams, dictionary=bigrams_dictionary, coherence='c_v')\n",
    "coherence_lda_bigrams = coherence_model_lda_bigrams.get_coherence()\n",
    "print(f\"\\nLDA bigrams coherence score: {coherence_lda_bigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a8e6ff-995b-4a5c-bd13-f765041c574a",
   "metadata": {},
   "source": [
    "## Model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd843c3b-d124-424a-ba72-2a0433979299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA perplexity: -7.234274714514751\n",
      "\n",
      "LDA bigrams perplexity: -7.465235588683538\n",
      "\n",
      "LDA coherence score: 0.452657714270358\n",
      "\n",
      "LDA bigrams coherence score: 0.44210836940752\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLDA perplexity: {lda_model.log_perplexity(chunk=document_term_matrix, total_docs=None)}\")   # a measure of how good the model is. lower the better.\n",
    "print(f\"\\nLDA bigrams perplexity: {lda_bigrams_model.log_perplexity(chunk=bigrams_document_term_matrix, total_docs=None)}\")   # a measure of how good the model is. lower the better.\n",
    "print(f\"\\nLDA coherence score: {coherence_lda}\")\n",
    "print(f\"\\nLDA bigrams coherence score: {coherence_lda_bigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ef079-e0c9-4c7a-8c27-1a876d20d7de",
   "metadata": {},
   "source": [
    "- The two models are similar in terms of perplexity and coherence, although the bigrams model has a slightly lower perplexity score, meaning that:\n",
    "    - the bigrams model is very slightly more able to measure the model's ability to predict a held-out word in a document\n",
    "    - Both models seem to be simialr in their assessing of how well words within a topic connect semantically and form meaningful themes.\n",
    "    - The scores for both models are good and **meet the key results of the project**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb13de7-3521-48e6-8677-dd895696e6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
