{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0f921a-870a-4a6c-9a09-fea1c15889d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..') # this resolves ImportError: attempted relative import with no known parent package\n",
    "\n",
    "# general DS packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cleaning and pre-processing\n",
    "from src.processing.text_cleaning import (normalize_text, process_contractions, remove_all_punctuation, remove_emojis, \n",
    "remove_html_unescape, remove_href_pattern, remove_digits, remove_extra_whitespace, remove_website_links)\n",
    "\n",
    "from src.processing.text_processing import (tokenize_comment, lemmatize_comment, remove_stop_words, remove_tiny_tokens, \n",
    "remove_tekken_character_names_from_tokens, part_of_speech, part_of_speech_tag, part_of_speech_dependency, part_of_speech_shape, \n",
    "part_of_speech_alpha, part_of_speech_is_stop)\n",
    "\n",
    "# modeling\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e164f05-1332-4e94-a6ca-4944b37c4df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import data from csv\n",
    "raw_data = pd.read_csv(\"data/raw/new_character_reveal_comments.csv\", )\n",
    "data = raw_data.copy()\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141cfccd-313a-43fc-aafa-5d84d2c5283a",
   "metadata": {},
   "source": [
    "# Clean and process dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a55b79d-44f3-4725-adab-d10c72c33fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.2 s, sys: 106 ms, total: 39.3 s\n",
      "Wall time: 39.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>totalReplyCount</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textStopWordsRemoved</th>\n",
       "      <th>textTokenized</th>\n",
       "      <th>textLemmatized</th>\n",
       "      <th>textTekkenCharactersRemoved</th>\n",
       "      <th>textProcessed</th>\n",
       "      <th>pos</th>\n",
       "      <th>posTag</th>\n",
       "      <th>posDependency</th>\n",
       "      <th>posShape</th>\n",
       "      <th>posAlpha</th>\n",
       "      <th>posStopWord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@silveriver9</td>\n",
       "      <td>2023-11-01 16:09:58+00:00</td>\n",
       "      <td>2023-11-01 16:10:43+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>first now where is lei wulong</td>\n",
       "      <td>lei wulong</td>\n",
       "      <td>[lei, wulong]</td>\n",
       "      <td>[lei, wulong]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[PROPN, NOUN]</td>\n",
       "      <td>[NNP, NN]</td>\n",
       "      <td>[compound, ROOT]</td>\n",
       "      <td>[xxx, xxxx]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[False, False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@faizaanjaved7150</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>already seen it you are getting less views now...</td>\n",
       "      <td>seen getting views bamco</td>\n",
       "      <td>[seen, getting, views, bamco]</td>\n",
       "      <td>[see, get, view, bamco]</td>\n",
       "      <td>[see, get, view, bamco]</td>\n",
       "      <td>see get view bamco</td>\n",
       "      <td>[VERB, VERB, NOUN, NOUN]</td>\n",
       "      <td>[VBN, VBG, NNS, NNS]</td>\n",
       "      <td>[ROOT, xcomp, dobj, dobj]</td>\n",
       "      <td>[xxxx, xxxx, xxxx, xxxx]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@TS-rw4lk</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>wow</td>\n",
       "      <td>wow</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>wow</td>\n",
       "      <td>[INTJ]</td>\n",
       "      <td>[UH]</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>[xxx]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@ALONCAK</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oww yeaah</td>\n",
       "      <td>oww yeaah</td>\n",
       "      <td>[oww, yeaah]</td>\n",
       "      <td>[oww, yeaah]</td>\n",
       "      <td>[oww, yeaah]</td>\n",
       "      <td>oww yeaah</td>\n",
       "      <td>[PROPN, PROPN]</td>\n",
       "      <td>[NNP, NNP]</td>\n",
       "      <td>[compound, ROOT]</td>\n",
       "      <td>[xxx, xxxx]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[False, False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@Rough_Estimates</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>135</td>\n",
       "      <td>14</td>\n",
       "      <td>i hope we get an angel version of jin</td>\n",
       "      <td>hope angel version jin</td>\n",
       "      <td>[hope, angel, version, jin]</td>\n",
       "      <td>[hope, angel, version, jin]</td>\n",
       "      <td>[hope, version]</td>\n",
       "      <td>hope version</td>\n",
       "      <td>[PROPN, PROPN, PROPN, PROPN]</td>\n",
       "      <td>[NNP, NNP, NNP, NNP]</td>\n",
       "      <td>[compound, compound, compound, ROOT]</td>\n",
       "      <td>[xxxx, xxxx, xxxx, xxx]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId  authorDisplayName                publishedAt  \\\n",
       "0  rDxrpSqYHD8       @silveriver9  2023-11-01 16:09:58+00:00   \n",
       "1  rDxrpSqYHD8  @faizaanjaved7150  2023-11-01 16:10:05+00:00   \n",
       "2  rDxrpSqYHD8          @TS-rw4lk  2023-11-01 16:10:05+00:00   \n",
       "3  rDxrpSqYHD8           @ALONCAK  2023-11-01 16:10:06+00:00   \n",
       "4  rDxrpSqYHD8   @Rough_Estimates  2023-11-01 16:10:06+00:00   \n",
       "\n",
       "                   updatedAt  likeCount  totalReplyCount  \\\n",
       "0  2023-11-01 16:10:43+00:00          4                4   \n",
       "1  2023-11-01 16:10:05+00:00          1                1   \n",
       "2  2023-11-01 16:10:05+00:00          0                0   \n",
       "3  2023-11-01 16:10:06+00:00          0                0   \n",
       "4  2023-11-01 16:10:06+00:00        135               14   \n",
       "\n",
       "                                         textDisplay  \\\n",
       "0                      first now where is lei wulong   \n",
       "1  already seen it you are getting less views now...   \n",
       "2                                                wow   \n",
       "3                                          oww yeaah   \n",
       "4              i hope we get an angel version of jin   \n",
       "\n",
       "       textStopWordsRemoved                  textTokenized  \\\n",
       "0                lei wulong                  [lei, wulong]   \n",
       "1  seen getting views bamco  [seen, getting, views, bamco]   \n",
       "2                       wow                          [wow]   \n",
       "3                 oww yeaah                   [oww, yeaah]   \n",
       "4    hope angel version jin    [hope, angel, version, jin]   \n",
       "\n",
       "                textLemmatized textTekkenCharactersRemoved  \\\n",
       "0                [lei, wulong]                          []   \n",
       "1      [see, get, view, bamco]     [see, get, view, bamco]   \n",
       "2                        [wow]                       [wow]   \n",
       "3                 [oww, yeaah]                [oww, yeaah]   \n",
       "4  [hope, angel, version, jin]             [hope, version]   \n",
       "\n",
       "        textProcessed                           pos                posTag  \\\n",
       "0                                     [PROPN, NOUN]             [NNP, NN]   \n",
       "1  see get view bamco      [VERB, VERB, NOUN, NOUN]  [VBN, VBG, NNS, NNS]   \n",
       "2                 wow                        [INTJ]                  [UH]   \n",
       "3           oww yeaah                [PROPN, PROPN]            [NNP, NNP]   \n",
       "4        hope version  [PROPN, PROPN, PROPN, PROPN]  [NNP, NNP, NNP, NNP]   \n",
       "\n",
       "                          posDependency                  posShape  \\\n",
       "0                      [compound, ROOT]               [xxx, xxxx]   \n",
       "1             [ROOT, xcomp, dobj, dobj]  [xxxx, xxxx, xxxx, xxxx]   \n",
       "2                                [ROOT]                     [xxx]   \n",
       "3                      [compound, ROOT]               [xxx, xxxx]   \n",
       "4  [compound, compound, compound, ROOT]   [xxxx, xxxx, xxxx, xxx]   \n",
       "\n",
       "                   posAlpha                   posStopWord  \n",
       "0              [True, True]                [False, False]  \n",
       "1  [True, True, True, True]  [False, False, False, False]  \n",
       "2                    [True]                       [False]  \n",
       "3              [True, True]                [False, False]  \n",
       "4  [True, True, True, True]  [False, False, False, False]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# clean\n",
    "df['textDisplay'] = df['textDisplay'].apply(normalize_text)\n",
    "df['textDisplay'] = df['textDisplay'].apply(process_contractions)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_website_links)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_html_unescape)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_emojis)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_digits)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_all_punctuation)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_href_pattern)\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_extra_whitespace)\n",
    "\n",
    "# process\n",
    "df[\"textStopWordsRemoved\"] = df[\"textDisplay\"].apply(remove_stop_words)\n",
    "df[\"textTokenized\"] = df['textStopWordsRemoved'].apply(tokenize_comment)\n",
    "df[\"textLemmatized\"] = df[\"textStopWordsRemoved\"].apply(lemmatize_comment)\n",
    "# remove short meaningless tokens from lemmatized tokens\n",
    "df[\"textLemmatized\"] = df['textLemmatized'].apply(remove_tiny_tokens)\n",
    "df[\"textTekkenCharactersRemoved\"] = df[\"textLemmatized\"].apply(remove_tekken_character_names_from_tokens)\n",
    "df[\"textProcessed\"] = df[\"textTekkenCharactersRemoved\"].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# part of speech operations\n",
    "df[\"pos\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech)\n",
    "df[\"posTag\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech_tag)\n",
    "df[\"posDependency\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech_dependency)\n",
    "df[\"posShape\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech_shape)\n",
    "df[\"posAlpha\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech_alpha)\n",
    "df[\"posStopWord\"] = df[\"textStopWordsRemoved\"].apply(part_of_speech_is_stop)\n",
    "                                 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af684d6e-d127-4554-b7c4-7f84c824a464",
   "metadata": {},
   "source": [
    "# Check for empty values after all the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faf321f2-b5a5-4b31-8477-7ef677b2eb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0 (empty string): see get view bamco\n",
      "Shape: (1769, 18)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Row 0 (empty string): {df['textProcessed'][0]}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b3d61d2-846c-4b2a-9ae7-0bfe0d9f53b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1769, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>totalReplyCount</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textStopWordsRemoved</th>\n",
       "      <th>textTokenized</th>\n",
       "      <th>textLemmatized</th>\n",
       "      <th>textTekkenCharactersRemoved</th>\n",
       "      <th>textProcessed</th>\n",
       "      <th>pos</th>\n",
       "      <th>posTag</th>\n",
       "      <th>posDependency</th>\n",
       "      <th>posShape</th>\n",
       "      <th>posAlpha</th>\n",
       "      <th>posStopWord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@faizaanjaved7150</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>already seen it you are getting less views now...</td>\n",
       "      <td>seen getting views bamco</td>\n",
       "      <td>[seen, getting, views, bamco]</td>\n",
       "      <td>[see, get, view, bamco]</td>\n",
       "      <td>[see, get, view, bamco]</td>\n",
       "      <td>see get view bamco</td>\n",
       "      <td>[VERB, VERB, NOUN, NOUN]</td>\n",
       "      <td>[VBN, VBG, NNS, NNS]</td>\n",
       "      <td>[ROOT, xcomp, dobj, dobj]</td>\n",
       "      <td>[xxxx, xxxx, xxxx, xxxx]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@TS-rw4lk</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>2023-11-01 16:10:05+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>wow</td>\n",
       "      <td>wow</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>wow</td>\n",
       "      <td>[INTJ]</td>\n",
       "      <td>[UH]</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>[xxx]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@ALONCAK</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oww yeaah</td>\n",
       "      <td>oww yeaah</td>\n",
       "      <td>[oww, yeaah]</td>\n",
       "      <td>[oww, yeaah]</td>\n",
       "      <td>[oww, yeaah]</td>\n",
       "      <td>oww yeaah</td>\n",
       "      <td>[PROPN, PROPN]</td>\n",
       "      <td>[NNP, NNP]</td>\n",
       "      <td>[compound, ROOT]</td>\n",
       "      <td>[xxx, xxxx]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[False, False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@Rough_Estimates</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>2023-11-01 16:10:06+00:00</td>\n",
       "      <td>135</td>\n",
       "      <td>14</td>\n",
       "      <td>i hope we get an angel version of jin</td>\n",
       "      <td>hope angel version jin</td>\n",
       "      <td>[hope, angel, version, jin]</td>\n",
       "      <td>[hope, angel, version, jin]</td>\n",
       "      <td>[hope, version]</td>\n",
       "      <td>hope version</td>\n",
       "      <td>[PROPN, PROPN, PROPN, PROPN]</td>\n",
       "      <td>[NNP, NNP, NNP, NNP]</td>\n",
       "      <td>[compound, compound, compound, ROOT]</td>\n",
       "      <td>[xxxx, xxxx, xxxx, xxx]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rDxrpSqYHD8</td>\n",
       "      <td>@kazamataurus337</td>\n",
       "      <td>2023-11-01 16:10:08+00:00</td>\n",
       "      <td>2023-11-01 16:10:08+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>so it begins</td>\n",
       "      <td>begins</td>\n",
       "      <td>[begins]</td>\n",
       "      <td>[begin]</td>\n",
       "      <td>[begin]</td>\n",
       "      <td>begin</td>\n",
       "      <td>[VERB]</td>\n",
       "      <td>[VBZ]</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>[xxxx]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId  authorDisplayName                publishedAt  \\\n",
       "0  rDxrpSqYHD8  @faizaanjaved7150  2023-11-01 16:10:05+00:00   \n",
       "1  rDxrpSqYHD8          @TS-rw4lk  2023-11-01 16:10:05+00:00   \n",
       "2  rDxrpSqYHD8           @ALONCAK  2023-11-01 16:10:06+00:00   \n",
       "3  rDxrpSqYHD8   @Rough_Estimates  2023-11-01 16:10:06+00:00   \n",
       "4  rDxrpSqYHD8   @kazamataurus337  2023-11-01 16:10:08+00:00   \n",
       "\n",
       "                   updatedAt  likeCount  totalReplyCount  \\\n",
       "0  2023-11-01 16:10:05+00:00          1                1   \n",
       "1  2023-11-01 16:10:05+00:00          0                0   \n",
       "2  2023-11-01 16:10:06+00:00          0                0   \n",
       "3  2023-11-01 16:10:06+00:00        135               14   \n",
       "4  2023-11-01 16:10:08+00:00          1                0   \n",
       "\n",
       "                                         textDisplay  \\\n",
       "0  already seen it you are getting less views now...   \n",
       "1                                                wow   \n",
       "2                                          oww yeaah   \n",
       "3              i hope we get an angel version of jin   \n",
       "4                                       so it begins   \n",
       "\n",
       "       textStopWordsRemoved                  textTokenized  \\\n",
       "0  seen getting views bamco  [seen, getting, views, bamco]   \n",
       "1                       wow                          [wow]   \n",
       "2                 oww yeaah                   [oww, yeaah]   \n",
       "3    hope angel version jin    [hope, angel, version, jin]   \n",
       "4                    begins                       [begins]   \n",
       "\n",
       "                textLemmatized textTekkenCharactersRemoved  \\\n",
       "0      [see, get, view, bamco]     [see, get, view, bamco]   \n",
       "1                        [wow]                       [wow]   \n",
       "2                 [oww, yeaah]                [oww, yeaah]   \n",
       "3  [hope, angel, version, jin]             [hope, version]   \n",
       "4                      [begin]                     [begin]   \n",
       "\n",
       "        textProcessed                           pos                posTag  \\\n",
       "0  see get view bamco      [VERB, VERB, NOUN, NOUN]  [VBN, VBG, NNS, NNS]   \n",
       "1                 wow                        [INTJ]                  [UH]   \n",
       "2           oww yeaah                [PROPN, PROPN]            [NNP, NNP]   \n",
       "3        hope version  [PROPN, PROPN, PROPN, PROPN]  [NNP, NNP, NNP, NNP]   \n",
       "4               begin                        [VERB]                 [VBZ]   \n",
       "\n",
       "                          posDependency                  posShape  \\\n",
       "0             [ROOT, xcomp, dobj, dobj]  [xxxx, xxxx, xxxx, xxxx]   \n",
       "1                                [ROOT]                     [xxx]   \n",
       "2                      [compound, ROOT]               [xxx, xxxx]   \n",
       "3  [compound, compound, compound, ROOT]   [xxxx, xxxx, xxxx, xxx]   \n",
       "4                                [ROOT]                    [xxxx]   \n",
       "\n",
       "                   posAlpha                   posStopWord  \n",
       "0  [True, True, True, True]  [False, False, False, False]  \n",
       "1                    [True]                       [False]  \n",
       "2              [True, True]                [False, False]  \n",
       "3  [True, True, True, True]  [False, False, False, False]  \n",
       "4                    [True]                       [False]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows with empty strings in the 'textProcessed' column as these will have nothing to pass to the vectorizer when we come to transforming the text input\n",
    "# to numerical input\n",
    "df = df[df[\"textProcessed\"].astype(str) != '']\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f384fa-97d1-4f74-8472-30015cb6f273",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# NMF model\n",
    "- At this point we have the text processed and avaibale in tokenized format and as a string\n",
    "- We now need to turn the text into numbers\n",
    "    - this can be done in a variety of ways e.g., TF-IDF, bag of words (which we previously used gensim to create as part of the LDA model)\n",
    "    - We're going to use TF-IDF to create the features\n",
    "- Once the features are created we can then create a topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576d4d4-de3c-4b6b-8ac1-6b22762615d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the words into numbers\n",
    "texts = df['textProcessed']\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=3,  # ignore words that appear in less than 3 of the youtube comments\n",
    "    max_df=0.85,   # ignore words that appear in more than 85% of the comments\n",
    "    max_features=999,   # each word will become a feature, set the max we want\n",
    "    ngram_range=(1, 2),   # allow tf-idf weights for bigrams\n",
    "    preprocessor=' '.join   # the model will tokenize everything by default so we need to join the tokenized words\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273d414-9418-4c70-b545-b5d3f5710977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform the text input\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d45d85-81be-49f6-9a25-4d1a1ec4d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the NMF model\n",
    "nmf = NMF(\n",
    "    n_components=10,   # manually select number of topics\n",
    "    init='nndsvd'   # ‘nndsvd’ initialiser works well on sparse data\n",
    ").fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464fc42-3bbf-4421-b17d-eaf251e8f898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760456ce-128c-4faa-a12f-d78aed9eac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tfidf_vectorizer.transform(texts)\n",
    "W = nmf.components_\n",
    "H = nmf.transform(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c401b7a-1bd0-4823-b73f-5282d579e1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87bcc4-1fa4-4c09-912b-233d1c66e787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8d67c-a2fe-4e9d-8e32-abb07a426b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c158d0a6-cd8f-4cd4-8283-08966d466286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075b4b6-903c-4757-a838-3f7fd497acba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b01095-6a0b-4eb7-a089-b937a6e9abfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b0a25-a44b-4fc1-b7ce-13bb5b81c581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da36125-7b1d-4321-91fc-aa50684b9929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba94b7-8953-4826-bfb0-dbd107c501de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8abaf9-a7a8-4aa7-9789-2a1df6184cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17384e5-936f-4f21-88d8-8a1cb6e05cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d224850-dc82-4501-808a-70b9735170cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f78bdd-171a-4fdc-a728-c263748f2360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc4320-21ed-4d19-9f74-ffde14937d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829ed6a-5a7e-4b53-8817-68aa60bc190c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f494d-ac74-4eb6-a320-95b75ef3d914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396b4d5-8250-402c-970f-7de9621cd33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c519426-1ec8-494b-8ccf-bf9e4fbf7f7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3690d47-24ef-4ef8-b019-995b78b312b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d70043-ebd3-429c-8506-00d363368282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
